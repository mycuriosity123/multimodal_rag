# multimodal_rag
analysing pdf file containing tables , images and text

input file here we used is latest quartely results of persistent systems

multimodal LLM used here is LLava:13b
embedding model from ollama : mxbai-embed-large

references:  https://blog.langchain.dev/semi-structured-multi-modal-rag/

cookbook : https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb

persistentsystems quartely results: https://www.bseindia.com/xml-data/corpfiling/AttachHis/f3835e32-ee21-4495-a679-666c454aa2fc.pdf

youtube ref: https://www.youtube.com/playlist?list=PLQxDHpeGU14D6dm0rmAXhdLeLYlX2zk7p
